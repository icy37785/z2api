# OpenAI å…¼å®¹ API ä»£ç† for Z.ai

ä¸€ä¸ªä¸º Z.ai GLM æ¨¡å‹æä¾› OpenAI å…¼å®¹ API æ¥å£çš„é«˜æ€§èƒ½ä»£ç†æœåŠ¡å™¨ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ”„ å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼
- ğŸš€ æ”¯æŒæµå¼å’Œéæµå¼å“åº”
- ğŸ§  æ”¯æŒå¤šç§ GLM æ¨¡å‹ï¼ˆGLM-4.5, GLM-4.5-thinking, GLM-4.5-search, GLM-4.5v ç­‰ï¼‰
- ğŸ–¼ï¸ æ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
- ğŸ› ï¸ æ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰
- ğŸ” æ”¯æŒè”ç½‘æœç´¢åŠŸèƒ½
- ğŸ’ª é«˜æ€§èƒ½ä¼˜åŒ–ï¼ˆè¿æ¥æ± ã€å¯¹è±¡æ± ã€å¹¶å‘æ§åˆ¶ï¼‰
- ğŸ“Š å†…ç½®æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå˜é‡

| å˜é‡å | æè¿° | é»˜è®¤å€¼ | å¿…éœ€ |
|--------|------|--------|------|
| `UPSTREAM_TOKEN` | Z.ai è®¿é—®ä»¤ç‰Œ | - | âŒ |
| `API_KEY` | å®¢æˆ·ç«¯ API å¯†é’¥ | `sk-tbkFoKzk9a531YyUNNF5` | âŒ |
| `PORT` | æœåŠ¡ç›‘å¬ç«¯å£ | `8080` | âŒ |
| `DEBUG_MODE` | è°ƒè¯•æ¨¡å¼ | `true` | âŒ |

### æœ¬åœ°è¿è¡Œ

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export UPSTREAM_TOKEN="ä½ çš„Z.aiè®¿é—®ä»¤ç‰Œ"

# è¿è¡ŒæœåŠ¡å™¨
go run main.go
```

### Docker éƒ¨ç½²

```dockerfile
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY . .
RUN go build -o main .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=builder /app/main .
CMD ["./main"]
```

### ä½¿ç”¨æ‰“åŒ…å¥½çš„ Docker é•œåƒéƒ¨ç½²

`docker pull ghcr.io/icy37785/openai-compatible-api-proxy-for-z:main`

## ğŸ“– æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹åç§° | è¯´æ˜ |
|---------|------|
| `glm-4.5` | æ ‡å‡†å¯¹è¯æ¨¡å‹ |
| `glm-4.5-thinking` | æ”¯æŒæ€è€ƒè¿‡ç¨‹çš„æ¨¡å‹ |
| `glm-4.5-search` | æ”¯æŒè”ç½‘æœç´¢çš„æ¨¡å‹ |
| `glm-4.5-air` | è½»é‡ç‰ˆæ¨¡å‹ |
| `glm-4.5v` | å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ”¯æŒå›¾ç‰‡ï¼‰ |

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Python (OpenAI SDK)

```python
import openai

client = openai.OpenAI(
    api_key="sk-tbkFoKzk9a531YyUNNF5",  # ä½¿ç”¨é…ç½®çš„APIå¯†é’¥
    base_url="http://localhost:8080/v1"  # ä»£ç†æœåŠ¡å™¨åœ°å€
)

# åŸºç¡€å¯¹è¯
response = client.chat.completions.create(
    model="glm-4.5",
    messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]
)
print(response.choices[0].message.content)

# æµå¼å“åº”
stream = client.chat.completions.create(
    model="glm-4.5",
    messages=[{"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### JavaScript/Node.js

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'sk-tbkFoKzk9a531YyUNNF5',
  baseURL: 'http://localhost:8080/v1'
});

const completion = await client.chat.completions.create({
  model: 'glm-4.5',
  messages: [{ role: 'user', content: 'ä½ å¥½' }],
});

console.log(completion.choices[0].message.content);
```

### cURL

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-tbkFoKzk9a531YyUNNF5" \
  -d '{
    "model": "glm-4.5",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "stream": true
  }'
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### å¤šæ¨¡æ€å¯¹è¯ (GLM-4.5v)

```python
response = client.chat.completions.create(
    model="glm-4.5v",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
            {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
        ]
    }]
)
```

### æ€è€ƒæ¨¡å¼ (GLM-4.5-thinking)

```python
response = client.chat.completions.create(
    model="glm-4.5-thinking",
    messages=[{"role": "user", "content": "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸç†"}]
)

# å“åº”åŒ…å«æ¨ç†è¿‡ç¨‹
print("æ€è€ƒè¿‡ç¨‹:", response.choices[0].message.reasoning_content)
print("æœ€ç»ˆå›ç­”:", response.choices[0].message.content)
```

### è”ç½‘æœç´¢ (GLM-4.5-search)

```python
response = client.chat.completions.create(
    model="glm-4.5-search",
    messages=[{"role": "user", "content": "æœ€è¿‘æœ‰ä»€ä¹ˆé‡è¦çš„ç§‘æŠ€æ–°é—»ï¼Ÿ"}]
)
```

### å‡½æ•°è°ƒç”¨

```python
response = client.chat.completions.create(
    model="glm-4.5",
    messages=[{"role": "user", "content": "ä»Šå¤©åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "åŸå¸‚åç§°"}
                }
            }
        }
    }]
)
```

## âš¡ æ€§èƒ½ç‰¹æ€§

- **è¿æ¥æ± å¤ç”¨**: ä¼˜åŒ–çš„ HTTP å®¢æˆ·ç«¯é…ç½®ï¼Œæ”¯æŒé«˜å¹¶å‘
- **å†…å­˜ä¼˜åŒ–**: å¯¹è±¡æ± å‡å°‘ GC å‹åŠ›ï¼Œé¢„åˆ†é…ç¼“å†²åŒº
- **å¹¶å‘æ§åˆ¶**: æ™ºèƒ½é™æµï¼Œé˜²æ­¢èµ„æºè€—å°½
- **æµå¼å¤„ç†**: é«˜æ•ˆçš„ SSE æµå¤„ç†ï¼Œå®æ—¶å“åº”
- **ç›‘æ§æ—¥å¿—**: å†…ç½®æ€§èƒ½ç»Ÿè®¡å’Œåˆ†å±‚æ—¥å¿—ç³»ç»Ÿ

## ğŸ“Š ç›‘æ§

æœåŠ¡å™¨æä¾›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§ä¿¡æ¯ï¼š

```
[INFO] è¯·æ±‚å®Œæˆ - æ¨¡å‹: glm-4.5, æ¨¡å¼: streaming, è€—æ—¶: 2.1s, tokens: 150
```

## ğŸ”§ éƒ¨ç½²å»ºè®®

### Render éƒ¨ç½²

1. Fork æ­¤ä»“åº“
2. åœ¨ Render åˆ›å»ºæ–°çš„ Web Service
3. è¿æ¥ GitHub ä»“åº“
4. è®¾ç½®ç¯å¢ƒå˜é‡ `UPSTREAM_TOKEN`
5. éƒ¨ç½²å®Œæˆ

### Railway éƒ¨ç½²

```bash
# å®‰è£… Railway CLI
npm install -g @railway/cli

# éƒ¨ç½²
railway login
railway init
railway add
railway deploy
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ†æ”¯
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

MIT License

## âš ï¸ å…è´£å£°æ˜

æœ¬é¡¹ç›®ä¸ºç¬¬ä¸‰æ–¹å¼€å‘ï¼Œä¸ Z.ai å®˜æ–¹æ— å…³ã€‚ä½¿ç”¨å‰è¯·ç¡®ä¿éµå®ˆç›¸å…³æœåŠ¡æ¡æ¬¾ã€‚

---

**ğŸ”— ç›¸å…³é“¾æ¥**
- [Z.ai å®˜ç½‘](https://chat.z.ai)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)